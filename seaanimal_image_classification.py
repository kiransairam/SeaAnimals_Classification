# -*- coding: utf-8 -*-
"""SeaAnimal_Image_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I7jaEUgD0aZ0Kxtl5624Wuvc_sZaly0u

#**Sea Animal Image ClassificationðŸŒŠ**
[link text](https://https://www.kaggle.com/datasets/vencerlanz09/sea-animals-image-dataste?select=Turtle_Tortoise)

**Below Link is Dataset**

[link text](https://https://www.kaggle.com/datasets/vencerlanz09/sea-animals-image-dataste/download?datasetVersionNumber=5)

**Aquatic environments have been the birthplace of most life forms, with the oceans providing approximately 90% of the world's living space in terms of volume. The first known vertebrates were fish, exclusively found in water. Some fish evolved into amphibians, capable of surviving both on land and in water for parts of the day. Among amphibians, certain subgroups, including sea turtles, seals, manatees, and whales, further evolved into reptiles and mammals. Plant life, such as kelp and other algae, support some underwater habitats. The ocean's food chain relies on phytoplankton, crucial primary producers.**


**Aquatic environments are home to a diverse array of organisms, including microscopic phytoplankton, which serve as the base of the ocean food chain. Phytoplankton are important primary producers, using energy from the sun to produce organic matter through photosynthesis. They are an essential food source for a wide range of organisms, from zooplankton to large predators like sharks and whales. And We are in this project to detect the 23 different types of sea animals,using the Neural Network models.**

**Variable or Sea AnimalNames:-**

Clams
Corals
Crabs
Dolphin
Eel
Fish
JellyFish
Lobster
Nudibranchs
Octopus
Otter
Penguin
Puffers
Sea Rays
Sea Urchins
Seahorse
Seal
Sharks
Shrimp
Squid
Starfish
Turtle_Tortoise
Whale

# **Works we did in this project**

##1. Packages to import
##2. Undersampling
##3. Importing and setting up the Dataset & Data Preprocessing
##4. Visualizing images from the dataset

##5.**Models**
###A. Baseline Model
###B. BaseLine with Multiple Layers and Dropout Layer
###C. Depth wise separable and Residual Block
###D. Transfer Learning using Feature Extraction
###E. Transfer Learning Using ResNet101



##6. Fine Tuning
##7. Evaluate Model and Analyse Results
##8. Showing missclassified images in the validaiton data
##9. Making predictions on the Test Data
##10. Classification Reports and Confusion Matrix

##**Packges to import**
"""

!pip install tensorflow

# Commented out IPython magic to ensure Python compatibility.
import zipfile, os
import shutil
import math
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import load_model
from tensorflow.keras.models import Model
import tensorflow.keras.utils

import keras
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras import backend as K
import itertools
#from keras.layers.normalization import BatchNormalization
from tensorflow.keras.layers import BatchNormalization
from keras.utils.np_utils import to_categorical

#from keras.optimizers import Adam
from tensorflow.keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score

from tensorflow.keras import layers
from tensorflow.keras import optimizers
from tensorflow.keras.optimizers import schedules

# %matplotlib inline
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
from glob import glob
import seaborn as sns
from PIL import Image
np.random.seed(123)
from sklearn.preprocessing import label_binarize
from sklearn.metrics import confusion_matrix
import itertools


from imutils import paths
import random
import math
from pathlib import Path
from pathlib import PurePath
from PIL import Image
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

from keras.models import Model
import matplotlib.pyplot as plt
from keras.optimizers import Adam
from keras.applications import MobileNet
from sklearn.metrics import confusion_matrix
from keras.layers.core import Dense, Activation
from keras.metrics import categorical_crossentropy
from sklearn.model_selection import train_test_split
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.mobilenet import preprocess_input
from tensorflow.keras.preprocessing import image_dataset_from_directory

"""##**Importing and setting up the Dataset & Data Preprocessing**"""

import os
import zipfile
from google.colab import drive

# Mount your Google Drive
drive.mount('/content/drive')

# Set the path to your zip file in your Drive
zip_path = '/content/drive/MyDrive/Datasets/archive.zip'

# Set the path to the directory where you want to extract the images
extract_path = '/content/drive/MyDrive/final_dL'

# Unzip the file and extract the images
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Define the directories where your images are located
data_dir = '/content/drive/MyDrive/final_dL'

# Define the directories for your training and validation sets
train_dir = '/content/drive/MyDrive/tvt/train'
val_dir = '/content/drive/MyDrive/tvt/val'
test_dir = '/content/drive/MyDrive/tvt/test'

# Create the train and validation directories
if not os.path.exists(train_dir):
    os.makedirs(train_dir)
if not os.path.exists(val_dir):
    os.makedirs(val_dir)
if not os.path.exists(test_dir):
    os.makedirs(test_dir)

# Loop over each class directory in the data directory
for class_dir in os.listdir(data_dir):
    class_path = os.path.join(data_dir, class_dir)

    # Create the train, validation, and test directories for this class
    train_class_dir = os.path.join(train_dir, class_dir)
    val_class_dir = os.path.join(val_dir, class_dir)
    test_class_dir = os.path.join(test_dir, class_dir)
    if os.path.exists(train_class_dir):
        shutil.rmtree(train_class_dir)
    if os.path.exists(val_class_dir):
        shutil.rmtree(val_class_dir)
    if os.path.exists(test_class_dir):
        shutil.rmtree(test_class_dir)
    os.makedirs(train_class_dir)
    os.makedirs(val_class_dir)
    os.makedirs(test_class_dir)

    # Loop over the images in this class directory
    images = os.listdir(class_path)
    num_train = int(len(images) * 0.6)  # Use 60% of images for training
    num_val = int(len(images) * 0.2)  # Use 20% of images for validation
    train_images = images[:num_train]
    val_images = images[num_train:num_train+num_val]
    test_images = images[num_train+num_val:]

    # Move the training images to the train directory
    for image in train_images:
        src_path = os.path.join(class_path, image)
        dst_path = os.path.join(train_class_dir, image)
        if os.path.isdir(src_path):
            shutil.copytree(src_path, dst_path)
        else:
            shutil.copy(src_path, dst_path)

    # Move the validation images to the val directory
    for image in val_images:
        src_path = os.path.join(class_path, image)
        dst_path = os.path.join(val_class_dir, image)
        if os.path.isdir(src_path):
            shutil.copytree(src_path, dst_path)
        else:
            shutil.copy(src_path, dst_path)

    # Move the test images to the test directory
    for image in test_images:
        src_path = os.path.join(class_path, image)
        dst_path = os.path.join(test_class_dir, image)
        if os.path.isdir(src_path):
            shutil.copytree(src_path, dst_path)
        else:
            shutil.copy(src_path, dst_path)

# Count images in train set
for category in os.listdir(train_dir):
    category_dir = os.path.join(train_dir, category)
    count = len(os.listdir(category_dir))
    print(f"Train set: {count} images in category {category}")

# Count images in test set
for category in os.listdir(test_dir):
    category_dir = os.path.join(test_dir, category)
    count = len(os.listdir(category_dir))
    print(f"Test set: {count} images in category {category}")

# Count images in test set
for category in os.listdir(val_dir):
    category_dir = os.path.join(val_dir, category)
    count = len(os.listdir(category_dir))
    print(f"Val set: {count} images in category {category}")

import tensorflow as tf

# Define the input size of the images
IMG_SIZE = 224

# Define the batch size
BATCH_SIZE = 32

# Create the train dataset
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    train_dir,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE)

# Create the validation dataset
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    val_dir,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE)

# Create the validation dataset
test_ds = tf.keras.preprocessing.image_dataset_from_directory(
    test_dir,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE)

print(train_ds)

image_dir = Path(data_dir)

# Get filepaths and labels
filepaths = list(image_dir.glob(r'**/*.JPG')) + list(image_dir.glob(r'**/*.jpg')) + list(image_dir.glob(r'**/*.png')) + list(image_dir.glob(r'**/*.PNG'))

labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))

filepaths = pd.Series(filepaths, name='Filepath').astype(str)
labels = pd.Series(labels, name='Label')

# Concatenate filepaths and labels
image_df = pd.concat([filepaths, labels], axis=1)

import PIL
from pathlib import Path
from PIL import UnidentifiedImageError

path = Path("../input/sea-animals-image-dataste").rglob("*.jpg")
for img_p in path:
    try:
        img = PIL.Image.open(img_p)
    except PIL.UnidentifiedImageError:
            print(img_p)

# Separate in train and test data
train_df, test_df = train_test_split(image_df, test_size=0.2, shuffle=True, random_state=42)

train_datagen = ImageDataGenerator(rescale=1./255,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2,
                                  shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True, fill_mode = 'nearest',
    validation_split=0.2) # set validation split

train_images = train_datagen.flow_from_directory(
    data_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    subset='training') # set as training data

validation_images = train_datagen.flow_from_directory(
    data_dir , # same directory as training data
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    subset='validation') # set as validation data

test_images = train_datagen.flow_from_directory(
    data_dir , # same directory as training data
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    subset='validation') # set as validation data

"""##**Visualizing images from the dataset**"""

# Display 23 picture of the dataset with their labels
random_index = np.random.randint(0, len(image_df), 23)
fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(10, 10),
                        subplot_kw={'xticks': [], 'yticks': []})

for i, ax in enumerate(axes.flat):
    ax.imshow(plt.imread(image_df.Filepath[random_index[i]]))
    ax.set_title(image_df.Label[random_index[i]])
plt.tight_layout()
plt.show()



"""##**Baseline Model**

**To begin constructing the CNN model, we start with an input layer that contains information regarding the shape and type of data that the model will be processing. Unlike the Dense layer, the image does not require flattening for the conv2D layer. We incorporated multiple blocks that feature progressively larger filters, each consisting of a convolutional layer, a ReLU activation function, a batch normalisation layer, and a max pooling layer. Following these blocks is a GlobalAveragePooling layer, culminating in an output layer that utilises the softmax activation function.**
"""

def build_baseline(input_shape, filters, dropout_rate):
    #Configuring the model architecture

    #input layer for getting the input image
    input = keras.Input(shape=input_shape)

    #rescaling layer for rescaling pixels to [0,1] range
    x = layers.experimental.preprocessing.Rescaling(1./255)(input)

    for filter in filters:
        #A block of two conv+batchnorm+relu  layers for extracting features
        x = layers.Conv2D(filters=filter, kernel_size=3, padding="same", use_bias=False)(x)
        x = layers.BatchNormalization()(x)
        x = layers.ReLU()(x)
        #x = layers.Dropout(dropout_rate)(x)

        #max pooling for downsampling
        x = layers.MaxPooling2D(pool_size=2, padding="same")(x)

    #Global Average pooling. This will get an input of shape (height, width, channels) the average of each feature map and returns a vector of size channels.
    x = layers.GlobalAveragePooling2D()(x)

    #The final output layer has one neuron with sigmoid activation to output the probability of the target class (cate or dog whichever is labeled as one)
    output = layers.Dense(23)(x)

    #create a model and set its input and output and return it
    model = keras.Model(inputs=input, outputs=output)
    return model

dropout_rate = 0.3
baseline = build_baseline(input_shape=(224,224,3), filters=[32,64,128,256], dropout_rate=dropout_rate)
print(baseline.summary())

# Set the learning rate schedule for the optimizer
initial_learning_rate = 1e-3
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=initial_learning_rate,
    decay_steps=1000,
    decay_rate=0.9)

"""**Loss function:** This function is used to evaluate the model's performance on images with known labels by calculating the difference between the observed and expected labels. For our multiclass classification task, we utilized the "categorical crossentropy" loss function. Our label variable was encoded using category encoding.

**Optimizer method:** The optimizer function is a critical component as it iteratively improves parameters like kernel values, bias of neurons, and weights to minimize the loss. The Adam optimizer was chosen for our model because it combines the benefits of two existing stochastic gradient descent modifications.

**Metric Function:** We employed the "accuracy" metric function to evaluate our model's performance. Unlike the loss function, the metric results are not used to train the model.

**We implemented two techniques during training to improve model performance:
Early stopping: This technique halts training if the validation loss fails to improve by at least 0.0001 after 10 epochs.**

**Model Checkpoint:** This method is used in conjunction with early stopping and enables continuous saving of the model during training after each epoch. We specify the file path and set the arguments save_best_only=True and monitor="val_loss". These parameters instruct the callback to only save a new file, overwriting any prior files, when the current value of the val_loss metric is lower than any previous value during training

"""

# Compiling the model
opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
baseline.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'], optimizer=opt)

# Callback for early stopping. Stop the training if the validation_loss does not improve after 10 epochs
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, min_delta=1e-4, restore_best_weights=True)

# Save the checkpointed model in your google drive cnn_lab directory: "drive/MyDrive/cnn_lab/baseline_checkpoint" .
checkpoint = keras.callbacks.ModelCheckpoint(filepath="drive/MyDrive/fil_dl/baseline_checkpoint", save_best_only=True, monitor="val_loss")

# Train the model
history = baseline.fit(
    train_ds,
    validation_data=val_ds,
    epochs=50,
    verbose=1,
    callbacks=[early_stopping, checkpoint]
)

"""we can see that the model was trained on a dataset using 50 epochs. In the first epoch, the model had a loss of 2.44 and an accuracy of 0.27 on the training set, and a validation loss of 2.20 and validation accuracy of 0.43.

As the epochs progress, we can see that the training loss decreases and the accuracy increases, indicating that the model is learning from the data. However, the validation loss remains high and the validation accuracy does not improve significantly, indicating that the model is overfitting on the training data and not generalizing well to new data.
"""

#Function to plot model's validation loss and validation accuracy
def plot_model_accuracy_loss(history):
    plt.figure(figsize=(12,6))

    #getting train and validation accuracies
    train_acc_CNN = history.history['accuracy']
    val_acc_CNN = history.history['val_accuracy']

    #getting train and validation losses
    train_loss_CNN = history.history['loss']
    val_loss_CNN = history.history['val_loss']
    epochs = range(1, len(train_loss_CNN) + 1)

    #plotting the training and validation accurracies
    plt.plot(epochs, train_acc_CNN, 'b', label='Training acc')
    plt.plot(epochs, val_acc_CNN, 'r', label='Validation acc')
    plt.title('Training and validation accuracy for CNN')
    plt.legend()
    plt.figure()

    plt.figure(figsize=(12,6))

    #plotting the train and validaiton losses
    plt.plot(epochs, train_loss_CNN, 'b', label='Training loss')
    plt.plot(epochs, val_loss_CNN, 'r', label='Validation loss')
    plt.title('Training and validation loss for CNN')

    plt.legend()
    plt.show()

#getting train and validation accuracies
plot_model_accuracy_loss(history)

"""## **BaseLine with Multiple Layers and Dropout Layer**

**Adding more layers can help extract additional features from the data, but only to a certain extent. Beyond a certain point, adding more layers can lead to overfitting, where the model becomes too closely fitted to the training data and loses its ability to generalize to new data. Overfitting can result in issues such as false positives.**

**To address this problem, dropout layers are commonly used in CNN training. Dropout layers prevent overfitting by randomly dropping out some nodes during training, which reduces the impact of any one batch of training data on the learning process. Dropout layers are typically more effective when placed between fully connected/Dense layers. In this case, dropout can be applied after the GlobalAveragePooling layer.**

**Another technique that can help prevent overfitting is batch normalization, which normalizes the output of a previous activation layer by re-centering and rescaling it. This can help avoid overfitting by reducing the covariance shift between layers during training. By combining dropout and batch normalization, we can effectively reduce overfitting and improve the performance of the model.**
"""

def build_baseline_with_multiple_layers(input_shape, filters):

  #Configuring the model architecture

  #input layer for getting the input image
  input = keras.Input(shape=input_shape)

  #rescaling layer for rescalign pixels to [0,1] range
  x = layers.experimental.preprocessing.Rescaling(1./255)(input)


  for filter in filters:
    #A block conv+batchnorm+relu  layers for extractign features
    x =layers.Conv2D(filters=filter, kernel_size=3, padding="same", use_bias=False)(x)
    x= layers.BatchNormalization()(x)
    x= layers.ReLU()(x)
    x =layers.Conv2D(filters=filter, kernel_size=3, padding="same", use_bias=False)(x)
    x= layers.BatchNormalization()(x)
    x= layers.ReLU()(x)
    x =layers.Conv2D(filters=filter, kernel_size=3, padding="same", use_bias=False)(x)
    x= layers.BatchNormalization()(x)
    x= layers.ReLU()(x)


    #max pooling for downsampling
    x = layers.MaxPooling2D(pool_size=2, padding="same")(x)

  #Global Average pooling. This will get an input of shape (height, width, channels) the average of each feature map and returns a vector of size channels.
  x = layers.GlobalAveragePooling2D()(x)

  # Dropout Layer
  x = layers.Dropout(0.5)(x)

  #The final output layer has one neuron with sigmoid activation to output the probability of the target class ( cate or dog whichever is labled as one)
  output=layers.Dense(23)(x)
  #create a model and set its input and output and return it
  model = keras.Model(inputs=input, outputs=output)
  return model

baseline_with_multilayers=build_baseline_with_multiple_layers(input_shape=(224,224,3), filters=[32,64,128, 256])
print(baseline_with_multilayers.summary())

lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-3,
    decay_steps=1000,
    decay_rate=0.9)

#compiling the model
opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
baseline_with_multilayers.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'], optimizer=opt)

#callback for early stopping. stop the training if the validation_loss does not improve after 10 epochs
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-4, restore_best_weights=True)

#save the checkpointed model in your google drive cnn_lab directory: "drive/MyDrive/cnn_lab/baseline_checkpoint" .
checkpoint= keras.callbacks.ModelCheckpoint( filepath="drive/MyDrive/fil_dl/baseline_checkpoint",save_best_only=True, monitor="val_loss")

history = baseline_with_multilayers.fit(
            train_ds,
            validation_data = val_ds,
            epochs = 50,
            verbose = 1,
            callbacks=[early_stopping, checkpoint])

#Function to plot model's validation loss and validation accuracy
def plot_model_accuracy_loss(history):
    plt.figure(figsize=(12,6))

    #getting train and validation accuracies
    train_acc_CNN = history.history['accuracy']
    val_acc_CNN = history.history['val_accuracy']

    #getting train and validation losses
    train_loss_CNN = history.history['loss']
    val_loss_CNN = history.history['val_loss']
    epochs = range(1, len(train_loss_CNN) + 1)

    #plotting the training and validation accurracies
    plt.plot(epochs, train_acc_CNN, 'b', label='Training acc')
    plt.plot(epochs, val_acc_CNN, 'r', label='Validation acc')
    plt.title('Training and validation accuracy for CNN')
    plt.legend()
    plt.figure()

    plt.figure(figsize=(12,6))

    #plotting the train and validaiton losses
    plt.plot(epochs, train_loss_CNN, 'b', label='Training loss')
    plt.plot(epochs, val_loss_CNN, 'r', label='Validation loss')
    plt.title('Training and validation loss for CNN')

    plt.legend()
    plt.show()

#getting train and validation accuracies
plot_model_accuracy_loss(history)

"""In the first epoch, the model achieved a training accuracy of 0.59 and a validation accuracy of 0.48, with a corresponding training loss of 0.43 and a validation loss of 1.83. As training continues, the training accuracy increases, while the validation accuracy fluctuates. The training loss decreases, but the validation loss exhibits some fluctuations.

Overall, the validation accuracy is much lower than the training accuracy, which indicates that the model is overfitting to the training data. The model seems to be improving in the first few epochs, but the improvement slows down as training progresses, and the validation accuracy fails to improve.

##**Depth wise separable and Residual Block**

**The ResNet architecture introduced the residual connection technique in 2015, which adds the input of a layer or block of layers back to its output, allowing the model to bypass the block if it increases training error. This residual link acts as a data shortcut around potentially harmful or noisy blocks, enabling error gradient information from early layers to pass through a deep network without noise.**

**In contrast to regular convolution, a depth-wise separable convolution decomposes a standard convolution into depthwise and pointwise convolutions, making it much more parameter-efficient and computationally efficient while having comparable representational capacity.**

**Keras provides layers for building depth-wise separable convolutions using SeparableConv2D. By replacing Conv2D layers with SeparableConv2D layers that have more parameter efficiency and adding residual connections, CNNs can be made more powerful. Repeating blocks of many convolution layers (not just one) are also commonly used in CNNs, as seen in recent architectures like ResNet and VGGNet.**
"""

def residual_block(x, filter):

  residual = x
  # x goes through a block consisting of two conv2d+batchnorm+reul as well as a max pooling

  # x = layers.MaxPooling2D(pool_size=2, padding="same")(x)
  x = layers.SeparableConv2D(filters=filter, kernel_size=3, padding="same", use_bias=False )(x)
  x= layers.BatchNormalization()(x)
  x= layers.ReLU()(x)
  x = layers.Dropout(0.2)(x)

  x = layers.SeparableConv2D(filters=filter, kernel_size=3, padding="same", use_bias=False)(x)
  x= layers.BatchNormalization()(x)
  x= layers.ReLU()(x)
  x = layers.Dropout(0.2)(x)

  x = layers.SeparableConv2D(filters=filter, kernel_size=3, padding="same", use_bias=False)(x)
  x= layers.BatchNormalization()(x)
  x= layers.ReLU()(x)
  x = layers.Dropout(0.2)(x)

  x = layers.SeparableConv2D(filters=filter, kernel_size=3, padding="same", use_bias=False)(x)
  x= layers.BatchNormalization()(x)
  x= layers.ReLU()(x)
  x = layers.Dropout(0.2)(x)

  x = layers.MaxPooling2D(pool_size=2, padding="same")(x)


  # After going through the above block x now has "filter" channels and its feature map is downsampled to half by max pooling
  #so we need to use a 1*1 convolution with stride=2 to downsample residual and change its numebr of channels to "filter".
  residual = layers.Conv2D(filters=filter, kernel_size=1, strides=2, use_bias=False)(residual)

  #Having batchnormalization layer as usual after convolution
  residual= layers.BatchNormalization()(residual)


  x = layers.add([x, residual])
  return x


def build_baseline_residual(input_shape, filters):

  #input layer for getting the input image
  input = keras.Input(shape=input_shape)

  #Add the data_augmentation layers here:
  #x=data_augmentation(input)

  #rescaling layer for rescalign pixels to [0,1] range
  x = layers.experimental.preprocessing.Rescaling(1./255)(input)

  # stack a set of residual blocks
  for filter in filters:
    x= residual_block(x, filter)

  #Global Average pooling. This will get an input of shape (height, width, channels) the average of each feature map and returns a vector of size channels.
  x = layers.GlobalAveragePooling2D()(x)



  #The final output layer has one neuron with sigmoid activation to output the probability of the target class ( cat or dog whichever is labled as one)
  output=layers.Dense(23)(x)

  #create a model and set its input and output and return it
  model = keras.Model(inputs=input, outputs=output)
  return model

baseline_residual=build_baseline_residual(input_shape=(224,224,3), filters=[32,64,128,256, 512])
print(baseline_residual.summary())

#compiling the model

lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-3,
    decay_steps=1000,
    decay_rate=0.9)


#callback for early stopping. stop the training if the validation_loss does not improve after 10 epochs
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-4, restore_best_weights=True)

opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
baseline_residual.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'], optimizer=opt)

checkpoint= keras.callbacks.ModelCheckpoint( filepath="drive/MyDrive/fil_dl/baseline_residual_checkpointt",save_best_only=True, monitor="val_loss")

history = baseline_residual.fit(
            train_ds,
            validation_data = val_ds,
            epochs = 50,
            verbose = 1,
            callbacks=[early_stopping, checkpoint])

"""Depth-wise separable convolution is a technique used in deep learning to reduce the number of parameters in a convolutional neural network (CNN) by decomposing the standard convolution operation into two simpler operations: depth-wise convolution and point-wise convolution. This results in a more computationally efficient network while maintaining the accuracy of the model."""

#Function to plot model's validation loss and validation accuracy
def plot_model_accuracy_loss(history):
    plt.figure(figsize=(12,6))

    #getting train and validation accuracies
    train_acc_CNN = history.history['accuracy']
    val_acc_CNN = history.history['val_accuracy']

    #getting train and validation losses
    train_loss_CNN = history.history['loss']
    val_loss_CNN = history.history['val_loss']
    epochs = range(1, len(train_loss_CNN) + 1)

    #plotting the training and validation accurracies
    plt.plot(epochs, train_acc_CNN, 'b', label='Training acc')
    plt.plot(epochs, val_acc_CNN, 'r', label='Validation acc')
    plt.title('Training and validation accuracy for CNN')
    plt.legend()
    plt.figure()

    plt.figure(figsize=(12,6))

    #plotting the train and validaiton losses
    plt.plot(epochs, train_loss_CNN, 'b', label='Training loss')
    plt.plot(epochs, val_loss_CNN, 'r', label='Validation loss')
    plt.title('Training and validation loss for CNN')

    plt.legend()
    plt.show()

#getting train and validation accuracies
plot_model_accuracy_loss(history)

"""## **Transfer Learning using Feature Extraction**

**Using a pretrained network is a common and highly effective approach to deep learning when working with small image datasets. A pretrained network is one that has been previously trained on a large dataset, typically for a large-scale image classification task. If the original dataset is sufficiently large and diverse, the hierarchical features learned by the pretrained network can serve as a generic model of the visual world, and these features can be useful for a wide range of computer vision problems, even if the new problems involve completely different classes than the original task.**

**One common technique for using a pretrained network is feature extraction. This involves taking the convolutional base (i.e., the convolutional layers and possibly batch normalization layers) of a well-known architecture (such as VGG, ResNet, Inception, DenseNet, etc.) that has been pretrained on ImageNet, running the new dataset through it to extract features, and then training a new classifier using these extracted features.**
"""

#instantiate a DenseNet121 model trained on ImageNet dataset
from tensorflow.keras.applications import DenseNet169
conv_base = DenseNet169(weights='imagenet',
include_top=False,
input_shape=(224, 224, 3))

#freeze the weight of the convolutional base
conv_base.trainable=False

# get the summary of the model to view its architecture
conv_base.summary()

def build_pretrained(input_shape):

  #Configuring the model architecture

  #input layer for getting the input image
  input = keras.Input(shape=input_shape)

  #Using the pre-trained conv_base
  x = conv_base(input)

  #Global Average pooling. This will get an input of shape (height, width, channels) the average of each feature map and returns a vector of size channels.
  x = layers.GlobalAveragePooling2D()(x)

  #The final output layer has one neuron with softmax activation
  output=layers.Dense(23, activation='softmax')(x)

  #The final output layer has one neuron with softmax activation
  output=layers.Dense(23, activation='softmax')(x)

  #The final output layer has one neuron with softmax activation
  output=layers.Dense(23, activation='softmax')(x)

  #create a model and set its input and output and return it
  model = keras.Model(inputs=input, outputs=output)
  return model

pretrained_model=build_pretrained(input_shape=(224, 224, 3))
print(pretrained_model.summary())

#compiling the model

lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-3,
    decay_steps=1000,
    decay_rate=0.9)

#callback for early stopping. stop the training if the validation_loss does not improve after 10 epochs
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-4, restore_best_weights=True)

opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
pretrained_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'], optimizer=opt)


history = pretrained_model.fit(
            train_ds,
            validation_data = val_ds,
            epochs = 50,
            verbose = 1,
            callbacks=[early_stopping])

#Function to plot model's validation loss and validation accuracy
def plot_model_accuracy_loss(history):
    plt.figure(figsize=(12,6))

    #getting train and validation accuracies
    train_acc_CNN = history.history['accuracy']
    val_acc_CNN = history.history['val_accuracy']

    #getting train and validation losses
    train_loss_CNN = history.history['loss']
    val_loss_CNN = history.history['val_loss']
    epochs = range(1, len(train_loss_CNN) + 1)

    #plotting the training and validation accurracies
    plt.plot(epochs, train_acc_CNN, 'b', label='Training acc')
    plt.plot(epochs, val_acc_CNN, 'r', label='Validation acc')
    plt.title('Training and validation accuracy for CNN')
    plt.legend()
    plt.figure()

    plt.figure(figsize=(12,6))

    #plotting the train and validaiton losses
    plt.plot(epochs, train_loss_CNN, 'b', label='Training loss')
    plt.plot(epochs, val_loss_CNN, 'r', label='Validation loss')
    plt.title('Training and validation loss for CNN')

    plt.legend()
    plt.show()

#getting train and validation accuracies
plot_model_accuracy_loss(history)

"""##**Transfer Learning Using ResNet101**

**Using transfer learning with ResNet101 involves taking the pre-trained ResNet101 model and removing the top layers (typically the fully connected layers) that were trained on ImageNet, a large-scale dataset of images with over 1,000 classes. The remaining layers of ResNet101 are then used as a convolutional feature extractor for the new task.**

**The next step is to add new layers to the model, which will be trained on the new dataset. The number of new layers and their architecture depend on the specific task. For example, if the new task is image classification, a few fully connected layers are added, followed by a softmax layer for outputting the predicted class probabilities.**

**Transfer learning using ResNet101 has several benefits. First, the pre-trained ResNet101 model already learned a lot of useful features from the ImageNet dataset, so it requires less training time and data to achieve good performance on the new task. Second, ResNet101 is a very deep and complex architecture that can learn complex and abstract features from images, making it suitable for a wide range of computer vision tasks.**
"""

resnet = tf.keras.applications.resnet.ResNet101(
    include_top=False,
    weights='imagenet',
    #input_tensor=,
    input_shape=(224,224,3),
    pooling=None,
    classes=1000,
    #**kwargs
)

print(resnet.summary())

def build_pretrained(input_shape):

  #Configuring the model architecture

  #input layer for getting the input image
  input = keras.Input(shape=input_shape)

  #Using the pre-trained conv_base
  x = resnet(input)

  #Global Average pooling. This will get an input of shape (height, width, channels) the average of each feature map and returns a vector of size channels.
  x = layers.GlobalAveragePooling2D()(x)

  #The final output layer has one neuron with softmax activation
  output=layers.Dense(23, activation='softmax')(x)

  #The final output layer has one neuron with softmax activation
  output=layers.Dense(23, activation='softmax')(x)

  #The final output layer has one neuron with softmax activation
  output=layers.Dense(23, activation='softmax')(x)

  #create a model and set its input and output and return it
  model = keras.Model(inputs=input, outputs=output)
  return model

pretrained_model=build_pretrained(input_shape=(224, 224, 3))
print(pretrained_model.summary())

lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-3,
    decay_steps=1000,
    decay_rate=0.9)

#callback for early stopping. stop the training if the validation_loss does not improve after 10 epochs
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-4, restore_best_weights=True)

opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
pretrained_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'], optimizer=opt)


history = pretrained_model.fit(
            train_ds,
            validation_data = val_ds,
            epochs = 20,
            verbose = 1,
            callbacks=[early_stopping])

#Function to plot model's validation loss and validation accuracy
def plot_model_accuracy_loss(history):
    plt.figure(figsize=(12,6))

    #getting train and validation accuracies
    train_acc_CNN = history.history['accuracy']
    val_acc_CNN = history.history['val_accuracy']

    #getting train and validation losses
    train_loss_CNN = history.history['loss']
    val_loss_CNN = history.history['val_loss']
    epochs = range(1, len(train_loss_CNN) + 1)

    #plotting the training and validation accurracies
    plt.plot(epochs, train_acc_CNN, 'b', label='Training acc')
    plt.plot(epochs, val_acc_CNN, 'r', label='Validation acc')
    plt.title('Training and validation accuracy for CNN')
    plt.legend()
    plt.figure()

    plt.figure(figsize=(12,6))

    #plotting the train and validaiton losses
    plt.plot(epochs, train_loss_CNN, 'b', label='Training loss')
    plt.plot(epochs, val_loss_CNN, 'r', label='Validation loss')
    plt.title('Training and validation loss for CNN')

    plt.legend()
    plt.show()

#getting train and validation accuracies
plot_model_accuracy_loss(history)

"""## **Fine Tuning**

**The code is a of fine-tuning a pre-trained convolutional neural network called conv_base. Fine-tuning involves unfreezing some layers of the pre-trained model and training them on a new dataset to improve its performance on a specific task.**
"""

#unfreez the convolution base
resnet.trainable = True

#set trainable to False for all layers except the last 9 , that is freeze the weights for all layers except the last 9 layers
for layer in resnet.layers[:-9]:
    layer.trainable=False

#set trainable to True for the convolutional layhers in the last 9 layers ( the last convolutional block in DENSENET121).
for layer in resnet.layers[-9:]:
    # we only want to unfreez the convolutional layers (batch normalizataion layers remain frozen)
    if layer.name.endswith("conv"):
      layer.trainable=True

print(pretrained_model.summary())

lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-3,
    decay_steps=1000,
    decay_rate=0.9)

#callback for early stopping. stop the training if the validation_loss does not improve after 10 epochs
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-4, restore_best_weights=True)

#compiling the model
opt = tf.keras.optimizers.Adam(learning_rate=1e-5)
pretrained_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'], optimizer=opt)

history = pretrained_model.fit(
            train_ds,
            validation_data = val_ds,
            epochs = 20,
            verbose = 1,
            callbacks=[early_stopping])

#Function to plot model's validation loss and validation accuracy
def plot_model_accuracy_loss(history):
    plt.figure(figsize=(12,6))

    #getting train and validation accuracies
    train_acc_CNN = history.history['accuracy']
    val_acc_CNN = history.history['val_accuracy']

    #getting train and validation losses
    train_loss_CNN = history.history['loss']
    val_loss_CNN = history.history['val_loss']
    epochs = range(1, len(train_loss_CNN) + 1)

    #plotting the training and validation accurracies
    plt.plot(epochs, train_acc_CNN, 'b', label='Training acc')
    plt.plot(epochs, val_acc_CNN, 'r', label='Validation acc')
    plt.title('Training and validation accuracy for CNN')
    plt.legend()
    plt.figure()

    plt.figure(figsize=(12,6))

    #plotting the train and validaiton losses
    plt.plot(epochs, train_loss_CNN, 'b', label='Training loss')
    plt.plot(epochs, val_loss_CNN, 'r', label='Validation loss')
    plt.title('Training and validation loss for CNN')

    plt.legend()
    plt.show()

#getting train and validation accuracies
plot_model_accuracy_loss(history)

"""The training process seems to be going well. The model achieved a high accuracy of 0.9215 on the training set and 0.7584 on the validation set after 20 epochs, indicating that it learned to generalize well to unseen data. The loss also decreased significantly over the epochs.

## **Evaluate Model and Analyse Results**

## **Computing the accuracy on the test data**
"""

#compute the loss and accurracy on the test set using model.evaluate method
test_loss, test_acc = pretrained_model.evaluate(test_ds)
print('test acc:', test_acc)

"""The test accuracy of the model is 0.6938.

##**Showing misclassified images in the validation data**
"""

batch_size=32
test_size=3000
batch_index=0

true_labels_arr = np.array([])
pred_labels_arr = np.array([])

#Get each batch of images and labels in test dataset
for image_batch,label_batch in test_ds:

    if batch_index>math.ceil(test_size/batch_size):
        break;

    y_pred = pretrained_model.predict(image_batch)

    y_pred_label=np.argmax(y_pred, axis=1)

    #print(label_batch)
    #print(y_pred_label)

    true_labels_arr = np.concatenate((true_labels_arr, label_batch), axis=None)

    pred_labels_arr = np.concatenate((pred_labels_arr, y_pred_label), axis=None)

    #get the indices of the missclassified images in the batch
    errors = np.where(y_pred_label != label_batch)[0]

    #display the missclassified images together with their predicted probabilities
    for i in errors:
        print(f"True label is: {label_batch[i]}, CNN predicted {y_pred_label[i]} with confidence {y_pred[i]}")
        plt.imshow(image_batch[i].numpy().astype("uint8"))
        plt.show()

    #get the next batch
    batch_index=batch_index+1

"""##**Classification Reports and Confusion Matrix**"""

# Metrics
from sklearn.metrics import classification_report, confusion_matrix

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Predict the label of the test_images
pred = pretrained_model.predict(test_images)
pred = np.argmax(pred,axis=1)

# Map the label
labels = (train_images.class_indices)
labels = dict((v,k) for k,v in labels.items())
pred = [labels[k] for k in pred]

# Display the result
print(f'The first 5 predictions: {pred[:5]}')

# Display 25 random pictures from the dataset with their labels
random_index = np.random.randint(0, len(test_df) - 1, 15)
fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(25, 15),
                        subplot_kw={'xticks': [], 'yticks': []})

for i, ax in enumerate(axes.flat):
    ax.imshow(plt.imread(test_df.Filepath.iloc[random_index[i]]))
    if test_df.Label.iloc[random_index[i]] == pred[random_index[i]]:
      color = "green"
    else:
      color = "red"
    ax.set_title(f"True: {test_df.Label.iloc[random_index[i]]}\nPredicted: {pred[random_index[i]]}", color=color)
plt.show()
plt.tight_layout()

# Metrics
from sklearn.metrics import classification_report, confusion_matrix

y_test = list(test_df.Label)
print(classification_report(y_test, pred))

report = classification_report(y_test, pred, output_dict=True)
df = pd.DataFrame(report).transpose()
df

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

from keras.models import Model
import matplotlib.pyplot as plt
from keras.optimizers import Adam
from keras.applications import MobileNet
from sklearn.metrics import confusion_matrix
from keras.layers.core import Dense, Activation
from keras.metrics import categorical_crossentropy
from sklearn.model_selection import train_test_split
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.mobilenet import preprocess_input
from tensorflow.keras.preprocessing import image_dataset_from_directory

from sklearn.metrics import confusion_matrix

def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(15, 7), text_size=10, norm=False, savefig=False):


  # Create the confustion matrix
    cm = confusion_matrix(y_true, y_pred)
    cm_norm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis] # normalize it
    n_classes = cm.shape[0] # find the number of classes we're dealing with

    # Plot the figure and make it pretty
    fig, ax = plt.subplots(figsize=figsize)
    cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better
    fig.colorbar(cax)

    # Are there a list of classes?
    if classes:
        labels = classes
    else:
        labels = np.arange(cm.shape[0])
        # Label the axes
    ax.set(title="Confusion Matrix",
         xlabel="Predicted label",
         ylabel="True label",
         xticks=np.arange(n_classes), # create enough axis slots for each class
         yticks=np.arange(n_classes),
         xticklabels=labels, # axes will labeled with class names (if they exist) or ints
         yticklabels=labels)

    # Make x-axis labels appear on bottom
    ax.xaxis.set_label_position("bottom")
    ax.xaxis.tick_bottom()
    ### Added: Rotate xticks for readability & increase font size (required due to such a large confusion matrix)
    plt.xticks(rotation=90, fontsize=text_size)
    plt.yticks(fontsize=text_size)

     # Set the threshold for different colors
    threshold = (cm.max() + cm.min()) / 2.

    # Plot the text on each cell
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if norm:
            plt.text(j, i, f"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)",
                horizontalalignment="center",
                color="white" if cm[i, j] > threshold else "black",
                size=text_size)
        else:
            plt.text(j, i, f"{cm[i, j]}",
              horizontalalignment="center",
              color="white" if cm[i, j] > threshold else "black",
              size=text_size)

  # Save the figure to the current working directory
    if savefig:
        fig.savefig("confusion_matrix.png")

make_confusion_matrix(y_test, pred, list(labels.values()))

